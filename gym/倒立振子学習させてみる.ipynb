{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.model to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.model` for environment variables or `env.get_wrapper_attr('model')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.data to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.data` for environment variables or `env.get_wrapper_attr('data')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#環境の定義\n",
    "\n",
    "__credits__ = [\"Kallinteris-Andreas\"]\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import mujoco\n",
    "import mujoco.viewer\n",
    "import glfw\n",
    "# import keyboard\n",
    "import math\n",
    "import numpy as np\n",
    "import pytest\n",
    "import random\n",
    "from gymnasium import utils\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium.error import Error\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "\n",
    "class MyCartPoleEnv(MujocoEnv, utils.EzPickle):\n",
    "    \"\"\"\n",
    "    Gymansium.MujocoEnv`環境APIを使った，倒立振子の強化学習環境\\n\n",
    "    倒れないで立ち続けるタスク．\\n\n",
    "    mujoco envを継承しています．\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "            \"depth_array\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, xml_file=\"my_xmls/mycartpole.xml\", frame_skip=1, **kwargs):\n",
    "        utils.EzPickle.__init__(self, xml_file, frame_skip, **kwargs)\n",
    "\n",
    "        MujocoEnv.__init__(\n",
    "            self,\n",
    "            xml_file,\n",
    "            frame_skip=frame_skip,\n",
    "            observation_space=None,  # needs to be defined after\n",
    "            default_camera_config={},\n",
    "            camera_name=\"camera_name\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.metadata = { #またメタデータがある。インスタンス変数？\n",
    "            \"render_modes\": [\n",
    "                \"human\",\n",
    "                \"rgb_array\",\n",
    "                \"depth_array\",\n",
    "            ],\n",
    "            \"render_fps\": int(np.round(1.0 / self.dt)),\n",
    "        }\n",
    "\n",
    "        obs_size = 6 #self.data.qpos.size + self.data.qvel.size #たとえば，観測空間に位置と速度を入れたいのであれば，その数をここに入れる。次元数なので数え方は０始まりではない。\n",
    "        # self.wall_hit = 0\n",
    "        #マイクロマウスのパラメータ用変数の用意\n",
    "        self.reward_graph = []\n",
    "        self.epi_reward_graph = []\n",
    "        self.mouse_xpos_graph = []\n",
    "        self.mouse_ypos_graph = []\n",
    "        self.lastxpos_graph = []\n",
    "        self.lastypos_graph = []\n",
    "        self.mouse_angle_rad = math.pi/2\n",
    "        self.mouse_xpos = 0\n",
    "        self.mouse_ypos = 0\n",
    "        self.mouse_vel = 0\n",
    "        self.now_time = 0\n",
    "        self.past_time = 0\n",
    "        self.delta_t = 0.01\n",
    "        self.wheel_r = 0\n",
    "        self.gear = 9.0e-3\n",
    "        self.wheel_r = 0.0135\n",
    "        self.tread = 0.072\n",
    "        self.right_rotation_sum = 0\n",
    "        self.left_rotation_sum = 0\n",
    "        self.stepcount = 0\n",
    "        self.angsum_obs = 0\n",
    "        self.epi_reward = 0\n",
    "        self.obsLS_graph = []\n",
    "        self.obsLF_graph = []\n",
    "        self.obsRS_graph = []\n",
    "        self.obsRF_graph = []\n",
    "        self.range_reward_x_graph = []\n",
    "        self.range_reward_y_graph = []\n",
    "        self.xpos_graph = []\n",
    "        self.ypos_graph = []\n",
    "        self.firstxyzahyou = self.init_qpos[0:3]\n",
    "        self.firstquat = self.init_qpos[3:7]\n",
    "        self.first_ang = (0, 0, 90)\n",
    "        self.first_ang_graph = []\n",
    "        \n",
    "\n",
    "\n",
    "        self.observation_space = Box( #観測空間を定義．gymnasium.sapcesのBoxを使用することで簡単に定義できる．Boxは連続値を扱う．\n",
    "            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64 #float64型の，-無限から無限までの値を取る，11次元の観測空間を用意\n",
    "        )\n",
    "\n",
    "    # \n",
    "    def get_odom(self, model, data): #今のタイヤの回転角度と速度（角度と角速度）を取得．\n",
    "        \"\"\"\n",
    "            現在のタイヤの回転角度と速度（と角速度）を取得する．\\\\\n",
    "            data.actuator().length,velocityで取得できる値はactuatorのgear倍されているので，それで割ることによって打ち消す，\n",
    "            Returns:\n",
    "                odm_right = 右タイヤの回転角度[rad]（基準からの総量） \\\\\n",
    "                odm_left = 左タイヤの回転角度[rad]（基準からの総量） \\\\\n",
    "                (vel_left) = 左タイヤの回転角速度 (単位時間あたり) \\\\\n",
    "            \"\"\"\n",
    "        odm_right = data.actuator('right').length[0]/self.gear # 得た値をgearで割ったもの＝タイヤの角度(rad)\n",
    "        odm_left = data.actuator('left').length[0]/self.gear # ＝タイヤの角度\n",
    "        vel_left = data.actuator('left').velocity[0]/self.gear # ＝タイヤの回転角速度\n",
    "        return odm_right, odm_left\n",
    "    \n",
    "    def get_pulse_count(self, pre_odm_right, pre_odm_left, odm_right, odm_left): #回転量をパルスにする エンコーダの役割をする関数\n",
    "        \"\"\"\n",
    "            回転量の差から，出たパルスの数を求める． (エンコーダの役割をする関数)\\\\\n",
    "            \n",
    "            Returns:\n",
    "                right_pulse_num = 右タイヤのパルス数 \\\\\n",
    "                left_pulse_num = 左タイヤのパルス数\n",
    "            \"\"\"\n",
    "        \n",
    "        pre_right_rotation_sum = self.right_rotation_sum #１つまえの回転量取得\n",
    "        pre_left_rotation_sum = self.left_rotation_sum\n",
    "        self.right_rotation_sum += (odm_right - pre_odm_right)/(2*math.pi) *4096 #1周あたり4096段階で，細かい回転量を計測（分解能ほぼ無限のエンコーダ）（小数で、0から1で回転を表していたものを0から4096で表すようにした）\n",
    "        self.left_rotation_sum += (odm_left - pre_odm_left)/(2*math.pi) *4096 #1周あたり4096段階で，細かい回転量を計測（分解能ほぼ無限のエンコーダ）\n",
    "        right_pulse_num = int(self.right_rotation_sum) - int(pre_right_rotation_sum) #現在の回転量を足した後の回転合計値ー足す前の回転合計値　これのintが１以上なら、１パルス出たことになる。パルスが出なくても回転量の合計は溜まっていく\n",
    "        left_pulse_num = int(self.left_rotation_sum) - int(pre_left_rotation_sum)\n",
    "        # print(\"回転量をパルス段階に変換　この値の差が，実際に出たパルスとなる：\", right_pulse_num)\n",
    "        return right_pulse_num, left_pulse_num #出たパルスの数\n",
    "    \n",
    "    def get_odom_at_pulse(self, right_pulse_num, left_pulse_num): #パルス数を元に、パルス数を元に、ロボットの速度と回転角速度を求める（オドメトリによる推定を行う）．\n",
    "        \"\"\"\n",
    "            パルス数を元に、ロボットの速度と回転角速度を求める．（オドメトリによる推定を行う）\\\\\n",
    "            Returns:\n",
    "                mouse_vel = ロボットの推定速度 \\\\\n",
    "                left_pulse_num = ロボットの推定回転速度\n",
    "            \"\"\"\n",
    "        right_wheel_move = 2*math.pi * self.wheel_r * (right_pulse_num/4096) #２πｒ×回転数で移動距離を求める．/delta_t\n",
    "        left_wheel_move = 2*math.pi * self.wheel_r * (left_pulse_num/4096) #２πｒ×回転数で移動距離を求める．\n",
    "        right_wheel_vel = right_wheel_move/self.delta_t #移動距離から，速度を求める\n",
    "        left_wheel_vel = left_wheel_move/self.delta_t #移動距離から，速度を求める\n",
    "        mouse_vel = (right_wheel_vel + left_wheel_vel)/2 #左右平均が並進速度\n",
    "        mouse_radvel = (right_wheel_vel - left_wheel_vel)/self.tread #これがマウスの旋回角速度（rad/s）\n",
    "        return mouse_vel, mouse_radvel #これで，エンコーダによる各タイヤの推定速度が求められた\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "          シミュレーションを1step進める。 \\\\\n",
    "          Returns: \\\\\n",
    "            observation, 観測空間。その行動後のエージェントが見えている状態を返す \\\\\n",
    "            reward, 報酬の合計値 \\\\\n",
    "            terminated = False, タスクの早期達成または強制終了条件にひっかかったわけではないが、意味のない行動をし続ける場合に陥ったときなどにエピソードを終了するフラグ。本環境ではそういう場面はないため常にFalseとなる。 \\\\\n",
    "            truncated = タスクの早期達成または強制終了が起きたときにそれを示すもの。壁にぶつかったら強制終了が起きる。ぶつかったとき以外はFalse。 \\\\\n",
    "            info = （現状未確認。何かの情報が入っている？）\\\\\n",
    "        \"\"\"\n",
    "        #変数定義\n",
    "        terminated = False\n",
    "        wall_hit = 0\n",
    "        course_out = 0\n",
    "        goal = 0\n",
    "        died_reward = 0\n",
    "        # velocity_before = self.data.sensor(\"Veloci\").data[0] # シミュレーションを進める前の速度センサの値を取得\n",
    "        ####オドメトリ####\n",
    "        self.past_time = self.data.time #シミュレーション進める前の時間を取得\n",
    "        pre_odm_right, pre_odm_left = self.get_odom(self.model, self.data) #今のタイヤの回転角度と速度（角度と角速度）を取得．\n",
    "        \n",
    "        self.do_simulation(action, self.frame_skip)####################シミュレーション実行####################################################################################\n",
    "        self.now_time = self.data.time #1step進んだ後のシミュレーションの時間を取得\n",
    "        self.delta_t = self.now_time - self.past_time #シミュレーション1ステップにかかった時間を取得\n",
    "        self.past_time = self.now_time #シミュレーション前の時間の変数を更新\n",
    "        odm_right, odm_left = self.get_odom(self.model, self.data)# エンコーダをもう一度読み取る　（行動後の値を取得）\n",
    "        \n",
    "        right_pulse_num, left_pulse_num = self.get_pulse_count(pre_odm_right, pre_odm_left, odm_right, odm_left)# シミュレーション前後のエンコーダ値から，この一瞬に出たパルス数を算出し，\n",
    "        mouse_vel, mouse_rad_vel = self.get_odom_at_pulse(right_pulse_num, left_pulse_num) # パルス数から，移動速度と角速度の推定値を求める。　これで，エンコーダによるマウスの速度・角速度が推定できた\n",
    "        # 速度に時間を掛けて，移動距離を計算 まず向きを計算し，その後移動距離を求める\n",
    "        self.mouse_angle_rad += mouse_rad_vel * self.delta_t #マウスの角速度はパルスの左右差で求められる。現在の角度は角速度に時間をかけた角度変化量を合計していくと求まる。\n",
    "        self.mouse_xpos += mouse_vel * math.cos(self.mouse_angle_rad) * self.delta_t #向いている角度と、進んでいる速度の情報から、ロボットの現在のx座標を更新する。\n",
    "        self.mouse_ypos += mouse_vel * math.sin(self.mouse_angle_rad) * self.delta_t #y座標版\n",
    "        self.mouse_xpos_graph.append(self.mouse_xpos)\n",
    "        self.mouse_ypos_graph.append(self.mouse_ypos)\n",
    "        ####オドメトリ終了####\n",
    "\n",
    "        #velocity_after = self.data.sensor(\"Veloci\").data[0] # 今の速度センサの値を取得\n",
    "        #mouse_vel = velocity_after - velocity_before #速度センサの値による、ロボットの速度を求める,,,速度ー速度で、速度変化を求めているけどつかわなそう。なんだこれ\n",
    "        # if mouse_vel < 0:\n",
    "        #     3*mouse_vel\n",
    "        self.mouse_vel = mouse_vel #ロボットの推定速度値をインスタンス変数に保存しておく処理\n",
    "\n",
    "        hit = self.data.sensor(\"HB1\").data[0] #フォースセンサの値取得\n",
    "        truncated = False\n",
    "        # print(\"hit wall\", hit_wall_f, hit_wall_b)\n",
    "        if hit > 0: #センサがぶつかったことを検出したら倒れたということなので、\n",
    "            died_reward = 100\n",
    "            truncated = True\n",
    "\n",
    "        goal_pos = self.data.body(\"goal\").xpos[0:2] #xy座標のみ\n",
    "        robot_pos = self.data.body(\"torso\").xpos[0:2]\n",
    "        goal_range_reward = np.linalg.norm(robot_pos - goal_pos) #ゴールとの直線距離　ユークリッド距離を計算\n",
    "        survive_reward = 0.3\n",
    "\n",
    "        observation = self._get_obs(self.angsum_obs)\n",
    "        reward = survive_reward - 0* goal_range_reward - died_reward \n",
    "        # reward = wall_hit + course_out + range_reward_x-0.5 +range_reward_y-0.5 + ang_reward[2]-0.5 - 1* np.sum(np.square(action)) # 目標との誤差に関する報酬は-0.5をつけて遠すぎると罰になるようオフセットを加えた。制御入力がでかいほど罰が大きくなることによって無駄な動きを抑制。 self.wall_hitにしないほうが（インスタンス変数にしないほうが）見通しが良い気がしたのでそうした。\n",
    "        # print(\"wall_hit:\",wall_hit , \"course_out:\", course_out, \"range_reward:\", range_reward, \"ang_reward:\", ang_reward)\n",
    "        self.reward_graph.append(reward)\n",
    "        self.epi_reward += reward\n",
    "        info = {}\n",
    "        \n",
    "        \n",
    "            \n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        # print(\"reward\", reward)\n",
    "        reward = float(reward)\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    \n",
    "    def _get_obs(self, angsum_obs): #状態空間を取得 学習のループで呼ばれる用\n",
    "        angsum_obs = np.array([angsum_obs])\n",
    "        # position = self.data.qpos[0:7].flat.copy() #神の視点の位置情報取得\n",
    "\n",
    "\n",
    "        # velocity = self.data.sensor(\"Veloci\").data[0] # 今の速度を取る\n",
    "        gyro = np.array([self.data.sensor('gyro').data])\n",
    "        gyro_0 = np.array([gyro[0][0]])\n",
    "        gyro_1 = np.array([gyro[0][1]])\n",
    "        gyro_2 = np.array([gyro[0][2]])\n",
    "        accelerometer = np.array([self.data.sensor('Accel').data])\n",
    "        accelerometer_0 = np.array([accelerometer[0][0]])\n",
    "        accelerometer_1 = np.array([accelerometer[0][1]])\n",
    "        accelerometer_2 = np.array([accelerometer[0][2]])\n",
    "        mouse_angle_rad = np.array([self.mouse_angle_rad])\n",
    "        mouse_xpos = np.array([self.mouse_xpos])#オドメトリによる推定座標\n",
    "        mouse_ypos = np.array([self.mouse_ypos])#オドメトリによる推定座標\n",
    "        self.xpos_graph.append(mouse_xpos)\n",
    "        self.ypos_graph.append(mouse_ypos)\n",
    "        goal_pos = self.data.body(\"goal\").xpos[0:2] #xy座標のみ\n",
    "\n",
    "        # ゴールの位置と、自分のオドメトリの推定値と、各センサの値が観測できる\n",
    "        # return np.concatenate((goal_pos, mouse_xpos, mouse_ypos, mouse_angle_rad, accelerometer_0, accelerometer_1, accelerometer_2, gyro_0, gyro_1, gyro_2))\n",
    "        return np.concatenate((accelerometer_0, accelerometer_1, accelerometer_2, gyro_0, gyro_1, gyro_2))\n",
    "        \n",
    "    def reset_model(self):\n",
    "        qpos = self.init_qpos.copy() #初期位置を取得\n",
    "        qvel = self.init_qvel.copy() #初期速度を取得\n",
    "        goal_xpos = np.random.uniform(0.2, 0.9)\n",
    "        goal_ypos = np.random.uniform(0.2,0.9)\n",
    "                \n",
    "        qpos[-2] = goal_xpos\n",
    "        qpos[-1] = goal_ypos\n",
    "        self.stepcount = 0\n",
    "        \n",
    "\n",
    "        # leftwall_on = np.random.uniform(0, 1)\n",
    "        # rightwall_on = np.random.uniform(0, 1)\n",
    "        # forwardwall_on = np.random.uniform(0, 1)\n",
    "        # backwall_on = np.random.uniform(0, 1)\n",
    "        # if leftwall_on >= 0:\n",
    "        #     qpos[-1] = -0.4\n",
    "        # else:\n",
    "        #     qpos[-1] = 0\n",
    "        # if rightwall_on >= 0:\n",
    "        #     qpos[-2] = 0.4\n",
    "        # else:\n",
    "        #     qpos[-2] = 0\n",
    "        # if forwardwall_on >= 0:\n",
    "        #     qpos[-3] = 0.4\n",
    "        # else:\n",
    "        #     qpos[-3] = 0\n",
    "        # if backwall_on >= 0:\n",
    "        #     qpos[-4] = -0.4\n",
    "        # else:\n",
    "        #     qpos[-4] = 0\n",
    "\n",
    "        # x_values = [0.27, 1.07, 1.87]\n",
    "        # mouse_xposition_shuffle = random.choice(x_values)\n",
    "        # x_values = 0.27\n",
    "\n",
    "        # mouse_xposition_shuffle = self.course_list[np.random]\n",
    "        # mouse_yposition_shuffle = self.course_list[np.random]\n",
    "\n",
    "        #両方3のときはやり直しする処理\n",
    "        \n",
    "        # qpos[0] = mouse_xposition_shuffle\n",
    "        # qpos[0] = x_values\n",
    "        # noise = np.random.normal(0, 0.3)  # 平均0、標準偏差0.1のノイズ\n",
    "        # qpos[3] += np.random.normal(0, 0.03)#0.27 1.07\n",
    "        # qpos[1] = mouse_yposition_shuffle\n",
    "        self.set_state(qpos, qvel) #qposとqvelには，すべての位置と速度の値がならんでいるためそれをセットする#################\n",
    "        self.mouse_angle_rad = 0\n",
    "        self.mouse_xpos = 0\n",
    "        self.mouse_ypos = 0\n",
    "        self.angsum_obs = 0\n",
    "\n",
    "        self.epi_reward_graph.append(self.epi_reward) #収益のグラフ表示用\n",
    "        self.epi_reward = 0 #収益を数え終わったから０にリセットして次のエピソードの報酬を数えていくため\n",
    "\n",
    "        observation = self._get_obs(self.angsum_obs)\n",
    "        #必要に応じてオドメトリ値もリセットする必要がある\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def _get_reset_info(self):\n",
    "        return {\"works\": True}\n",
    "    \n",
    "    # from typing import Any, Dict, Optional, Tuple, Union\n",
    "    # def reset(\n",
    "    #     self,\n",
    "    #     *,\n",
    "    #     seed: Optional[int] = None,\n",
    "    #     options: Optional[dict] = None,\n",
    "    # ):\n",
    "    #     super().reset(seed=seed) #サブクラスにresetを書くとそちらだけが実行されるように隠蔽されてしまうので，スーパークラスのresetを実行して，前に定義した内容も実行する\n",
    "\n",
    "    #     self._reset_simulation()\n",
    "    #     print(\"リセットメソッドのオーバーライドができているか確認するためのprint文です．\")\n",
    "\n",
    "    #     ob = self.reset_model()\n",
    "    #     info = self._get_reset_info()\n",
    "\n",
    "    #     if self.render_mode == \"human\":\n",
    "    #         self.render()\n",
    "    #     return ob, info\n",
    "\n",
    "#＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃オプション。特に使わない＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃\n",
    "CHECK_ENV_IGNORE_WARNINGS = [\n",
    "    f\"\\x1b[33mWARN: {message}\\x1b[0m\"\n",
    "    for message in [\n",
    "        \"A Box observation space minimum value is -infinity. This is probably too low.\",\n",
    "        \"A Box observation space maximum value is infinity. This is probably too high.\",\n",
    "        \"For Box action spaces, we recommend using a symmetric and normalized space (range=[-1, 1] or [0, 1]). See https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html for more information.\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"frame_skip\", [1, 2, 3, 4, 5])\n",
    "def test_frame_skip(frame_skip):\n",
    "    \"\"\"verify that custom envs work with different `frame_skip` values\"\"\"\n",
    "    env = MyCartPoleEnv(frame_skip=frame_skip) #フレームスキップ設定の上書き？\n",
    "\n",
    "    # Test if env adheres to Gym API\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        check_env(env.unwrapped, skip_render_check=True)\n",
    "        env.close()\n",
    "    for warning in w:\n",
    "        if warning.message.args[0] not in CHECK_ENV_IGNORE_WARNINGS:\n",
    "            raise Error(f\"Unexpected warning: {warning.message}\")\n",
    "\n",
    "\n",
    "def test_xml_file():\n",
    "    \"\"\"Verify that the loading of a custom XML file works \\\\\n",
    "      訳：XMLファイルの読み込みがうまくいくことを確認する\"\"\"\n",
    "    relative_path = \"./tests/envs/mujoco/assets/walker2d_v5_uneven_feet.xml\"\n",
    "    env = MyCartPoleEnv(xml_file=relative_path).unwrapped\n",
    "    assert isinstance(env, MujocoEnv)\n",
    "    assert env.data.qpos.size == 9\n",
    "\n",
    "    full_path = os.getcwd() + \"/tests/envs/mujoco/assets/walker2d_v5_uneven_feet.xml\"\n",
    "    env = MyCartPoleEnv(xml_file=full_path).unwrapped\n",
    "    assert isinstance(env, MujocoEnv)\n",
    "    assert env.data.qpos.size == 9\n",
    "\n",
    "    # note can not test user home path (with '~') because github CI does not have a home folder　訳：ユーザーホームパス（'〜'を含む）をテストできないことに注意してください。github CIにはホームフォルダがありません\n",
    "\n",
    "\n",
    "def test_reset_info():\n",
    "    \"\"\"Verify that the environment returns info at `reset()`\"\"\"\n",
    "    env = MyCartPoleEnv()\n",
    "\n",
    "    _, info = env.reset()\n",
    "    assert info[\"works\"] is True\n",
    "#＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃オプションここまで＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃\n",
    "\n",
    "#＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃用意した環境クラスを使用し、学習済みモデルをロード＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃\n",
    "\n",
    "import gymnasium as gym\n",
    "gym.envs.registration.register(id='MyCartPoleEnv-v0',max_episode_steps=1000,entry_point=MyCartPoleEnv) #環境登録 idは環境の名前付け。 max_episode_steps=600, は1エピソードの最大ステップ数 entry_point=MouseRightEnv は環境クラスの指定\n",
    "\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import datetime\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "start_time = datetime.datetime.now().replace(microsecond=0) #学習にかかった時間を計測するために使用　小数の時間は切り捨て(microsecond=0)\n",
    "\n",
    "env = gym.make(\"MyCartPoleEnv-v0\", render_mode=\"human\") #環境クラスの作成。レンダーモードを選択 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create model\n",
    "# model = mujoco.MjModel.from_xml_path('../mouse_in_maze.xml')\n",
    "# data = mujoco.MjData(model)\n",
    "model = env.model\n",
    "data = env.data\n",
    "\n",
    "odom_right_graph = []\n",
    "mouse_correct_vel_graph = []\n",
    "\n",
    "gear = 9.0e-3\n",
    "wheel_r = 0.0135\n",
    "tread = 0.072\n",
    "mx = 0.0\n",
    "my = 0.0\n",
    "psi = 0.0\n",
    "past_odom_right = 0.0\n",
    "past_odom_left = 0.0\n",
    "# kp = 100\n",
    "# st_Kp = 1.55 #ダンピング3.6e-6のときの最適なゲイン\n",
    "st_Kp = 8.4\n",
    "rad_Kp = 0.15\n",
    "rad_vel_Kp = 2\n",
    "\n",
    "rad_Kd = 0.00001\n",
    "\n",
    "ki6 = 0.02\n",
    "kd = 0.09\n",
    "I_err_sum = 0\n",
    "d_grad = 0\n",
    "prev_err_rad = 0\n",
    "target_vel = 0\n",
    "target_rad = math.pi/2\n",
    "target_rad_vel = 0\n",
    "mjstep_count = 0\n",
    "\n",
    "right_pulse_num = 0\n",
    "right_pulse_num_graph = []\n",
    "left_pulse_num = 0\n",
    "left_pulse_num_graph = []\n",
    "\n",
    "right_rotation_sum = 0\n",
    "left_rotation_sum = 0\n",
    "\n",
    "mouse_vel = 0\n",
    "mouse_rad_vel = 0\n",
    "mouse_xpos = 0\n",
    "mouse_xpos_graph = []\n",
    "mouse_ypos = 0\n",
    "mouse_ypos_graph = []\n",
    "target_vel_graph = []\n",
    "err_vel_graph = []\n",
    "mouse_vel_graph = []\n",
    "\n",
    "viz_now = 0.0\n",
    "viz_past = 0.0\n",
    "now_time = 0\n",
    "past_time = 0\n",
    "turn_flag = 0\n",
    "x_vel = 0\n",
    "y_vel = 0\n",
    "z_vel = 0\n",
    "err = 0\n",
    "delta_t = 0.001 #############################################制御周期を変える場合はここも変える！！！！！！\n",
    "mouse_now_rad_xy = math.pi/2 #y軸方向を向いている\n",
    "I_err_rad_vel_sum = 0\n",
    "mouse_angle_rad = (math.pi/2)\n",
    "right_mot = 0\n",
    "left_mot = 0 #最初は0\n",
    "\n",
    "#デバッグ用グラフのためのリスト\n",
    "LRF_lf_graph = []\n",
    "LRF_ls_graph = []\n",
    "LRF_rs_graph = []\n",
    "LRF_rf_graph = []\n",
    "lf_sensor_graph = []\n",
    "rf_sensor_graph = []\n",
    "ls_sensor_graph = []\n",
    "rs_sensor_graph = []\n",
    "t_graph = []\n",
    "kakusokudo_graph = []\n",
    "rlkakusokudo_graph = []\n",
    "kakudo_graph = []\n",
    "final_kakudo_graph = []\n",
    "rlfinal_kakudo_graph = []\n",
    "rlkakudo_graph = []\n",
    "right_mot_graph = []\n",
    "left_mot_graph = []\n",
    "x_vel_graph = []\n",
    "velocimeter_graph = []\n",
    "err_graph = []\n",
    "err_rad_graph = []\n",
    "now_kakudo_graph = []\n",
    "now_kakudo_god_graph = []\n",
    "kakudo_err_graph = []\n",
    "target_kakudo_graph = []\n",
    "gyro_graph = []\n",
    "yzahyou = []\n",
    "timevals = []\n",
    "timevals5 = []\n",
    "timevals6 = []\n",
    "mouse_angle_rad_graph = []\n",
    "mouse_correct_rad_vel_graph = []\n",
    "mouse_rad_vel_graph = []\n",
    "err_rad_vel_graph = []\n",
    "target_rad_vel_graph = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cartpoletest4.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m\n\u001b[0;32m     20\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMyCartPoleEnv-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m,render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# video_path = \"./\"  # 保存先のpath\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# env = RecordVideo(env, video_path, video_length=500)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# env.model.opt.timestep = 0.01  # タイムステップを設定 RecordVideoする場合はそれ以降に書かないとerrorが出る\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# model = PPO(\"MlpPolicy\", env, verbose=0)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcartpoletest4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# render_freq = 1000  # 1000ステップごとにレンダリング\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# callback = CustomCallback(render_freq)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:680\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    678\u001b[0m     get_system_info()\n\u001b[1;32m--> 680\u001b[0m data, params, pytorch_variables \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_zip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_system_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo params found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:403\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[1;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_zip_file\u001b[39m(\n\u001b[0;32m    377\u001b[0m     load_path: Union[\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath, io\u001b[38;5;241m.\u001b[39mBufferedIOBase],\n\u001b[0;32m    378\u001b[0m     load_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     print_system_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    Load model data from a .zip archive\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m        and dict of pytorch variables\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:240\u001b[0m, in \u001b[0;36mopen_path_str\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;129m@open_path\u001b[39m\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_path_str\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m, verbose: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, suffix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m io\u001b[38;5;241m.\u001b[39mBufferedIOBase:\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    that the path exists.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:291\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    285\u001b[0m         path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# if opening was successful uses the open_path() function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m#   with corrections\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:272\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    270\u001b[0m             path, suffix \u001b[38;5;241m=\u001b[39m newpath, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:264\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m open_path(\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, mode, verbose, suffix)\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m suffix \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\pathlib.py:1044\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1043\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cartpoletest4.zip'"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC, PPO ,DQN\n",
    "# from sb3_contrib import RecurrentPPO\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import datetime\n",
    "\n",
    "# env = gym.make(\"ALE/Breakout-v5\",  render_mode=\"human\")\n",
    "# env = gym.make(\"CartPole-v1\",  render_mode=\"human\")\n",
    "# # env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)　こんな感じで，\n",
    "# model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "# model.learn(total_timesteps=200)\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "env = gym.make(\"MyCartPoleEnv-v0\",render_mode=\"human\")\n",
    "\n",
    "# video_path = \"./\"  # 保存先のpath\n",
    "# env = RecordVideo(env, video_path, video_length=500)\n",
    "# env.model.opt.timestep = 0.01  # タイムステップを設定 RecordVideoする場合はそれ以降に書かないとerrorが出る\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "model = PPO.load('cartpoletest4',env)\n",
    "# render_freq = 1000  # 1000ステップごとにレンダリング\n",
    "# callback = CustomCallback(render_freq)\n",
    "for i in range(10000):\n",
    "    model.learn(total_timesteps=20000)#progress_bar = True, callback=callback)\n",
    "\n",
    "#     # モデルのテスト\n",
    "#     obs, _ = env.reset()  # ここを修正\n",
    "    \n",
    "#     for unk in trange(10000):\n",
    "#         # 環境の描画\n",
    "#         env.render()\n",
    "\n",
    "#         # モデルの推論\n",
    "#         # print(\"本物のobs!\",obs)\n",
    "#         action, _ = model.predict(obs)\n",
    "#         # print(\"本物のaction\", action)\n",
    "\n",
    "#         # 1ステップ実行\n",
    "#         obs, reward, terminated, truncated, info = env.step(action) #以前の4要素のタプルから5要素のタプルに変更され，`observation, reward, terminated, truncated, info`という形式になった。\n",
    "#         # print(obs)\n",
    "\n",
    "\n",
    "#         # エピソード完了（終了または切り捨て）のチェック terminated(終了した)は目的を達成してエピソードを終了したことを表す，truncated(切り捨てられた)は，達成できずにエピソードが終了したことを表す\n",
    "#         if terminated or truncated:\n",
    "#             # print(obs)\n",
    "#             obs, _ = env.reset() # エピソードが終了したら、環境をリセット\n",
    "\n",
    "\n",
    "            \n",
    "    # #モデルの保存 (1)\n",
    "    model.save('cartpoletest4')\n",
    "    print(\"モデルを保存しました\")\n",
    "        # グラフを作成\n",
    "    plt.plot(env.epi_reward_graph)\n",
    "    plt.show()\n",
    "    # model.save('hidarimawari_housyuu_godsight_onlyxy_notight')\n",
    "\n",
    "# print(video_path)\n",
    "plt.title(\"各ステップの報酬値\", fontname=\"MS Gothic\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"ステップ数\", fontname=\"MS Gothic\")\n",
    "plt.ylabel(\"報酬値\", fontname=\"MS Gothic\")\n",
    "\n",
    "#前進関係のプロット\n",
    "t = list(range(len(env.reward_graph)))  # 0から始まるインデックスのリストを作成 エピソードの数分報酬（収益）がリストに保存されるので、その数を数えてリストにしただけ　データ数なら自動で出るから必要なかった\n",
    "plt.plot(t, env.reward_graph, linestyle='solid', label=\"報酬\")\n",
    "plt.legend()\n",
    "# 環境のクローズ\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:01<00:00, 765.10it/s] \n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import mujoco\n",
    "from stable_baselines3 import PPO\n",
    "import imageio\n",
    "from tqdm import trange\n",
    "\n",
    "def create_simulation_video(env_id, learned_model_path, timestep_interval, video_framerate, total_time, video_speed_level):\n",
    "    \"\"\"\n",
    "    Stable baselines3で学習させたモデルのmujoco上の推論の動画を作成する関数\n",
    "    Args:\n",
    "        env_id (str): 環境ID\n",
    "        learned_model_path (str): 学習済みモデルのパス\n",
    "        timestep_interval (float): シミュレーションのタイムステップ間隔 (秒) xmlに定義したtimestepに合わせる\n",
    "        video_framerate (int): ビデオのフレームレート (FPS)\n",
    "        total_time (float): シミュレーションの総時間 (秒)\n",
    "        video_speed_level (int): ビデオの再生速度 (倍)\n",
    "        \n",
    "    Returns:\n",
    "        env (gym.Env): 環境\n",
    "\n",
    "        使用例:\n",
    "        env = create_simulation_video(\n",
    "            env_id='MouseRightEnv-v0', #env_id='YourEnvironmentID',\n",
    "            learned_model_path='test', #model_path='your_model_path.zip', \n",
    "            timestep_interval=0.01,  # シミュレーションのタイムステップ間隔 (秒) xmlに定義したtimestepに合わせる\n",
    "            video_framerate=30,      # ビデオのフレームレート (FPS)\n",
    "            total_time=14.0,         # シミュレーションの総時間 (秒)\n",
    "            video_speed_level=3      # ビデオの再生速度 (倍) 実時間に対して何倍の速さで再生するかを決める。1倍ならそのまま、2倍なら2倍速で再生する\n",
    "        )\n",
    "    \"\"\"\n",
    "    # 環境とモデルのロード\n",
    "    env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    model = PPO.load(learned_model_path)\n",
    "    \n",
    "    \n",
    "\n",
    "    # 環境の初期化\n",
    "    obs, _ = env.reset()  # ここを修正\n",
    "    frames = []\n",
    "\n",
    "    # 設定\n",
    "    total_steps = int(total_time / timestep_interval)  # timestepと実時間の要求から総ステップ数を計算 \n",
    "    frame_capture_interval = int(1 / (timestep_interval * video_framerate))  # フレーム取得間隔を計算 要求のfpsの動画を作るには何秒ごとにスクショすればいいか計算している\n",
    "    frame_capture_interval = frame_capture_interval*video_speed_level  #動画の実時間に対して何倍の速さで再生するかを決める。1倍ならそのまま、2倍なら2倍速で再生する\n",
    "\n",
    "    # シミュレーション\n",
    "    for step in trange(total_steps):\n",
    "        # action, _states = model.predict(obs) #deterministicはTrueにすると、より決定論的になるそうです(?)\n",
    "        action = [0.05, 0.05]  # ここを修正\n",
    "        # print(\"action:\", action)\n",
    "        obs, rewards, terminated, truncated, info = env.step(action)\n",
    "        if step % frame_capture_interval == 0:\n",
    "            #シミュレーション内の様子をレンダーにセット\n",
    "            frame = env.render() #実際にセットしたやつの見た目を描画つまりデータで表す。そしてその描画データ（スクショ）をframe変数に保存\n",
    "            frames.append(frame) #ためていく\n",
    "        if truncated == True or terminated == True:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    # ビデオの保存\n",
    "    with imageio.get_writer('simulation_video.mp4', fps=video_framerate) as video:\n",
    "        for frame in frames:\n",
    "            video.append_data(frame)\n",
    "\n",
    "    env.close()\n",
    "    return env\n",
    "\n",
    "# 使用\n",
    "env = create_simulation_video(\n",
    "    env_id='MyCartPoleEnv-v0', #env_id='YourEnvironmentID',\n",
    "    learned_model_path='cartpoletest', #model_path='your_model_path.zip', \n",
    "    timestep_interval=0.01,  # シミュレーションのタイムステップ間隔 (秒)\n",
    "    video_framerate=10,       # ビデオのフレームレート (FPS)\n",
    "    total_time=15.0,           # シミュレーションの総時間 (秒)\n",
    "    video_speed_level=1       # ビデオの再生速度 (倍) 実時間に対して何倍の速さで再生するかを決める。1倍ならそのまま、2倍なら2倍速で再生する\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU will be used.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU will be used.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.6     |\n",
      "|    ep_rew_mean     | 337      |\n",
      "| time/              |          |\n",
      "|    fps             | 111      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 39.6        |\n",
      "|    ep_rew_mean          | 365         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004629122 |\n",
      "|    clip_fraction        | 0.0778      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.00385     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.06e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00722    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 9.78e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 39.6         |\n",
      "|    ep_rew_mean          | 365          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 111          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027229907 |\n",
      "|    clip_fraction        | 0.00835      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0258       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.94e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 1.02e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 41.4        |\n",
      "|    ep_rew_mean          | 382         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005406052 |\n",
      "|    clip_fraction        | 0.06        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.00841     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.02e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 8.99e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 45.6         |\n",
      "|    ep_rew_mean          | 421          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 109          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030057123 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0041       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.89e+03     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 1.02e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 49.9         |\n",
      "|    ep_rew_mean          | 462          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 110          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033132804 |\n",
      "|    clip_fraction        | 0.0182       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0022       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.19e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 1.04e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 50.4         |\n",
      "|    ep_rew_mean          | 466          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 116          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 123          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034692474 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.00156      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.84e+03     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 1.03e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 52.4         |\n",
      "|    ep_rew_mean          | 485          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035968758 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.000765     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.75e+03     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.01e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 53.2         |\n",
      "|    ep_rew_mean          | 492          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 143          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018921483 |\n",
      "|    clip_fraction        | 0.00361      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.000908     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.3e+03      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 1e+04        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50.7        |\n",
      "|    ep_rew_mean          | 469         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002203618 |\n",
      "|    clip_fraction        | 0.00879     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.00061     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.25e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.000814   |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 9.76e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51           |\n",
      "|    ep_rew_mean          | 472          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 170          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 132          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033006659 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.00132      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.63e+03     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00294     |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 8.95e+03     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 54.5       |\n",
      "|    ep_rew_mean          | 505        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 134        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00316934 |\n",
      "|    clip_fraction        | 0.015      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.028      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.32e+03   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00173   |\n",
      "|    std                  | 0.989      |\n",
      "|    value_loss           | 9.24e+03   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 58.3         |\n",
      "|    ep_rew_mean          | 540          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 194          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036150126 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0228       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.7e+03      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 9.57e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60.3         |\n",
      "|    ep_rew_mean          | 559          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 138          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012650576 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0363       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.47e+03     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000572    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 1.03e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60.9       |\n",
      "|    ep_rew_mean          | 565        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 218        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 140        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00214677 |\n",
      "|    clip_fraction        | 0.0133     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.4       |\n",
      "|    explained_variance   | 0.0913     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.3e+03    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.00292   |\n",
      "|    std                  | 0.978      |\n",
      "|    value_loss           | 9.16e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 59.6        |\n",
      "|    ep_rew_mean          | 553         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002538373 |\n",
      "|    clip_fraction        | 0.0063      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.0519      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.44e+03    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.000772   |\n",
      "|    std                  | 0.972       |\n",
      "|    value_loss           | 9.18e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 62.3         |\n",
      "|    ep_rew_mean          | 577          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039666216 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.066        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.55e+03     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00423     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 9.26e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 61.3         |\n",
      "|    ep_rew_mean          | 568          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 146          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036171027 |\n",
      "|    clip_fraction        | 0.00947      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0863       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.82e+03     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000984    |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 9.55e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 59.7         |\n",
      "|    ep_rew_mean          | 553          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 262          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 147          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022355318 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.169        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.6e+03      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 7.75e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 59.8        |\n",
      "|    ep_rew_mean          | 554         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002634082 |\n",
      "|    clip_fraction        | 0.00942     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.03e+03    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.000964   |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 7.57e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 64.4         |\n",
      "|    ep_rew_mean          | 597          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 283          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040306957 |\n",
      "|    clip_fraction        | 0.0413       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.429        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.79e+03     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00487     |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 8.34e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 68.5         |\n",
      "|    ep_rew_mean          | 635          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 292          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 154          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032842287 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.457        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.09e+03     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0039      |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 7.98e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 70           |\n",
      "|    ep_rew_mean          | 649          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 301          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 156          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035862457 |\n",
      "|    clip_fraction        | 0.011        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.467        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.67e+03     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 7.9e+03      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 72.5         |\n",
      "|    ep_rew_mean          | 672          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 311          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030688408 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.519        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.76e+03     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 7.47e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 73.5        |\n",
      "|    ep_rew_mean          | 682         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 320         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003388419 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.72e+03    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 7.26e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# モデルの推論\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 1ステップ実行\u001b[39;00m\n\u001b[0;32m     20\u001b[0m state, rewards, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:553\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    535\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:355\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    363\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make(\"InvertedDoublePendulum-v4\",  render_mode=\"human\")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=50000, progress_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "# モデルのテスト\n",
    "state = env.reset()\n",
    "for i in range(4000):\n",
    "    # 環境の描画\n",
    "    env.render()\n",
    "\n",
    "    # モデルの推論\n",
    "    action, _ = model.predict(state)\n",
    "\n",
    "    # 1ステップ実行\n",
    "    state, rewards, done, info = env.step(action)\n",
    "\n",
    "    # エピソード完了\n",
    "    if done:\n",
    "        break\n",
    "# モデルの保存 (1)\n",
    "model.save('test')\n",
    "# 環境のクローズ\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
